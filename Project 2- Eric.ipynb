{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "Clean Strings\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import time, random, re, pprint\n",
    "from itertools import islice, chain\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 177 ms, total: 1.22 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "train_input = pd.read_csv('train_input.csv')\n",
    "train_output = pd.read_csv('train_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;speaker_1&gt; seaworld ceo steps down amid tanki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;speaker_1&gt; strickland chargers owner dean spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       conversation\n",
       "0   0  <speaker_1> seaworld ceo steps down amid tanki...\n",
       "1   1  <speaker_1> strickland chargers owner dean spa..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "train_input.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delchars = ''.join(c for c in map(chr, range(256)) if not c.isalpha())\n",
    "delchars = ''.join(ch for ch in delchars if ch !=' ')\n",
    "\n",
    "def clean_text(paragraph):\n",
    "    output = re.sub('\\<.*?\\>','', paragraph) #delete tags (between <..>)\n",
    "    output = re.sub('\\@.*?\\s','', output) #delete usernames (words following @)\n",
    "    output = re.sub('\\n','', output) #delete anything following a slash\n",
    "    output = output.translate(str.maketrans('','',delchars))\n",
    "    return output\n",
    "\n",
    "def rem_stopwords(word_list): \n",
    "    return list(set(word_list) - set(stopwords.words('English')))\n",
    "\n",
    "count_stops = Counter(stopwords.words(\"English\")*100)\n",
    "\n",
    "def rem_stopwords_count(words_list):\n",
    "    return Counter(words_list) - count_stops\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemma_map(word_list):\n",
    "    return list(map(lambda y: wnl.lemmatize(y), word_list))\n",
    "\n",
    "def generate_features(dataframe):\n",
    "    dataframe[\"text\"] = dataframe[\"conversation\"].apply(lambda x: clean_text(x))\n",
    "    dataframe[\"words\"] = dataframe[\"text\"].apply(lambda x: x.split()) \n",
    "#    dataframe[\"words_lems\"] = dataframe[\"words\"].apply(lambda x: lemma_map(x)) # If we want to do lemmatization..\n",
    "#    dataframe[\"words_count\"] = dataframe[\"words_lems\"].apply(lambda x: rem_stopwords_count(x))\n",
    "    dataframe[\"words_count\"] = dataframe[\"words\"].apply(lambda x: rem_stopwords_count(x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 750 ms, total: 21.7 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_input = generate_features(train_input)\n",
    "train_input[\"output\"] = train_output[\"category\"]\n",
    "#strip out all common words (stopwords, which we import from the nltk package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>words_count</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;speaker_1&gt; seaworld ceo steps down amid tanki...</td>\n",
       "      <td>seaworld ceo steps down amid tanking revenues...</td>\n",
       "      <td>[seaworld, ceo, steps, down, amid, tanking, re...</td>\n",
       "      <td>{'propaganda': 1, 'sway': 1, 'coupled': 1, 'ta...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;speaker_1&gt; strickland chargers owner dean spa...</td>\n",
       "      <td>strickland chargers owner dean spanos and gol...</td>\n",
       "      <td>[strickland, chargers, owner, dean, spanos, an...</td>\n",
       "      <td>{'utkevinacee': 1, 'acee': 1, 'spanos': 2, 'in...</td>\n",
       "      <td>nfl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;speaker_1&gt; iniesta plays keepy uppy with one ...</td>\n",
       "      <td>iniesta plays keepy uppy with one leg man  yo...</td>\n",
       "      <td>[iniesta, plays, keepy, uppy, with, one, leg, ...</td>\n",
       "      <td>{'one': 2, 'let': 1, 'find': 1, 'youtube': 1, ...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;speaker_1&gt; chappie trailer #1 &lt;number&gt; hugh j...</td>\n",
       "      <td>chappie trailer   hugh jackman sci  fi comedy...</td>\n",
       "      <td>[chappie, trailer, hugh, jackman, sci, fi, com...</td>\n",
       "      <td>{'described': 1, 'anyone': 1, 'youtu': 1, 'tow...</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;speaker_1&gt; why the church of satan may get to...</td>\n",
       "      <td>why the church of satan may get to open your ...</td>\n",
       "      <td>[why, the, church, of, satan, may, get, to, op...</td>\n",
       "      <td>{'one': 1, 'long': 1, 'religious': 1, 'worship...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       conversation  \\\n",
       "0   0  <speaker_1> seaworld ceo steps down amid tanki...   \n",
       "1   1  <speaker_1> strickland chargers owner dean spa...   \n",
       "2   2  <speaker_1> iniesta plays keepy uppy with one ...   \n",
       "3   3  <speaker_1> chappie trailer #1 <number> hugh j...   \n",
       "4   4  <speaker_1> why the church of satan may get to...   \n",
       "\n",
       "                                                text  \\\n",
       "0   seaworld ceo steps down amid tanking revenues...   \n",
       "1   strickland chargers owner dean spanos and gol...   \n",
       "2   iniesta plays keepy uppy with one leg man  yo...   \n",
       "3   chappie trailer   hugh jackman sci  fi comedy...   \n",
       "4   why the church of satan may get to open your ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [seaworld, ceo, steps, down, amid, tanking, re...   \n",
       "1  [strickland, chargers, owner, dean, spanos, an...   \n",
       "2  [iniesta, plays, keepy, uppy, with, one, leg, ...   \n",
       "3  [chappie, trailer, hugh, jackman, sci, fi, com...   \n",
       "4  [why, the, church, of, satan, may, get, to, op...   \n",
       "\n",
       "                                         words_count    output  \n",
       "0  {'propaganda': 1, 'sway': 1, 'coupled': 1, 'ta...      news  \n",
       "1  {'utkevinacee': 1, 'acee': 1, 'spanos': 2, 'in...       nfl  \n",
       "2  {'one': 2, 'let': 1, 'find': 1, 'youtube': 1, ...    soccer  \n",
       "3  {'described': 1, 'anyone': 1, 'youtu': 1, 'tow...    movies  \n",
       "4  {'one': 1, 'long': 1, 'religious': 1, 'worship...  politics  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, Learn the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output\n",
       "hockey       20861\n",
       "movies       22409\n",
       "nba          18422\n",
       "news         21057\n",
       "nfl          20106\n",
       "politics     19694\n",
       "soccer       21363\n",
       "worldnews    21088\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.groupby(\"output\").size()\n",
    "#Size of each of the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Recursive algorithm to efficiently sum a list of counters\n",
    "def mergeSum(Counter_list):\n",
    "    if len(Counter_list) < 2:\n",
    "        return Counter_list.iloc[0]\n",
    "    \n",
    "    else: \n",
    "        mid = len(Counter_list)//2\n",
    "\n",
    "        lefthalf = mergeSum(Counter_list[:mid])\n",
    "        righthalf = mergeSum(Counter_list[mid:])\n",
    "\n",
    "        my_sum = lefthalf + righthalf\n",
    "        return my_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 2.22 s, total: 1min 14s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Total word Counter\n",
    "total_counter = mergeSum(train_input[\"words_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 2.15 s, total: 1min 15s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#Word Counter for each group\n",
    "groups_counter = train_input.groupby(\"output\")[\"words_count\"].apply(lambda x: mergeSum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107517"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6863865"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_wordcount = sum(total_counter.values())\n",
    "#Total number of words (not unique)\n",
    "total_wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 ms, sys: 396 ms, total: 499 ms\n",
      "Wall time: 585 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "total_word_freq = Counter({k:v/total_wordcount for k,v in total_counter.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "group_word_freq = {}\n",
    "groups_freq = groups_counter\n",
    "for label in train_input[\"output\"].unique():\n",
    "    group_wordcount = sum(groups_counter[label])\n",
    "    group_word_freq[label] = Counter({k:v/group_wordcount for k,v in groups_freq[label].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.9 s, sys: 75.5 ms, total: 4.97 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_value_dict(my_dict,key):\n",
    "    if key in my_dict: \n",
    "        return my_dict[key]\n",
    "    else:\n",
    "        return 0\n",
    "group_word_laplace = {}\n",
    "total_words = len(total_counter)\n",
    "for label in train_input[\"output\"].unique():\n",
    "    temp_group = groups_counter[label]\n",
    "    group_wordcount = sum(groups_counter[label])\n",
    "    #Conditional probability calculation with laplace smoothing\n",
    "    group_word_laplace[label] = Counter({k:(get_value_dict(temp_group,k)+1)/(v + total_words)for k,v in total_counter.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 s, sys: 151 ms, total: 3.96 s\n",
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Build a counter for the IDFs of each word in our corpus\n",
    "words_doc_list = [list(counter.keys()) for counter in list(train_input[\"words_count\"].values)]\n",
    "words_doc_counter = Counter(chain.from_iterable(set(x) for x in words_doc_list))\n",
    "num_docs = len(train_input)\n",
    "total_words_idf = Counter({k: np.log(num_docs / words_doc_counter[k]) for k in total_counter.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.17 s, sys: 58 ms, total: 5.23 s\n",
      "Wall time: 5.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_value_dict(my_dict,key):\n",
    "    if key in my_dict: \n",
    "        return my_dict[key]\n",
    "    else:\n",
    "        return 0\n",
    "group_word_idf = {}\n",
    "total_words = len(total_counter)\n",
    "for label in train_input[\"output\"].unique():\n",
    "    temp_group = groups_counter[label]\n",
    "    group_wordcount = sum(groups_counter[label])\n",
    "    #Conditional probability calculation with laplace smoothing\n",
    "    group_word_idf[label] = Counter({k:((get_value_dict(temp_group,k) + 1) / (v + total_words) * total_words_idf[k] )for k,v in total_counter.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53218\n"
     ]
    }
   ],
   "source": [
    "test_input = pd.read_csv('test_input.csv')\n",
    "print(len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.34 s, sys: 228 ms, total: 6.56 s\n",
      "Wall time: 6.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_input = generate_features(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;speaker_1&gt; philadelphia is decriminalizing ma...</td>\n",
       "      <td>philadelphia is decriminalizing marijuana pos...</td>\n",
       "      <td>[philadelphia, is, decriminalizing, marijuana,...</td>\n",
       "      <td>{'vice': 1, 'congress': 1, 'sway': 1, 'hopes':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;speaker_1&gt; david cameron pushes for repeal of...</td>\n",
       "      <td>david cameron pushes for repeal of u  k  s hu...</td>\n",
       "      <td>[david, cameron, pushes, for, repeal, of, u, k...</td>\n",
       "      <td>{'harper': 1, 'rights': 1, 'k': 1, 'australia'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       conversation  \\\n",
       "0   0  <speaker_1> philadelphia is decriminalizing ma...   \n",
       "1   1  <speaker_1> david cameron pushes for repeal of...   \n",
       "\n",
       "                                                text  \\\n",
       "0   philadelphia is decriminalizing marijuana pos...   \n",
       "1   david cameron pushes for repeal of u  k  s hu...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [philadelphia, is, decriminalizing, marijuana,...   \n",
       "1  [david, cameron, pushes, for, repeal, of, u, k...   \n",
       "\n",
       "                                         words_count  \n",
       "0  {'vice': 1, 'congress': 1, 'sway': 1, 'hopes':...  \n",
       "1  {'harper': 1, 'rights': 1, 'k': 1, 'australia'...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now, create a function to predict class for each text snippet \n",
    "categories = train_input[\"output\"].unique()\n",
    "class_priors = {}\n",
    "for category in categories:\n",
    "    class_priors[category] = train_input.groupby(\"output\").size()[category] / len(train_input)\n",
    "    \n",
    "total_words = len(total_counter)\n",
    "\n",
    "def get_conditional(word, category): \n",
    "    #function that gets around cases where we haven't seen the word before\n",
    "    if word in group_word_laplace[category]:\n",
    "        return group_word_laplace[category][word]\n",
    "    else: \n",
    "        return (1 / total_words)\n",
    "    \n",
    "def get_conditional_idf(word, category): \n",
    "    #function that gets around cases where we haven't seen the word before\n",
    "    if word in group_word_idf[category]:\n",
    "        return group_word_idf[category][word]\n",
    "    else: \n",
    "        return (1 / total_words)\n",
    "\n",
    "def predict_class(word_counter):\n",
    "    classes_prob = {}\n",
    "    for category in categories:\n",
    "        classes_prob[category] = 1\n",
    "        for k, v in word_counter.items():\n",
    "            classes_prob[category] *= (get_conditional(k,category) ** v)\n",
    "        classes_prob[category] *= class_priors[category]\n",
    "        #update with the prior class probability \n",
    "    return max(classes_prob, key = classes_prob.get) \n",
    "\n",
    "def predict_class_idf(word_counter):\n",
    "    classes_prob = {}\n",
    "    for category in categories:\n",
    "        classes_prob[category] = 1\n",
    "        for k, v in word_counter.items():\n",
    "            classes_prob[category] *= (get_conditional_idf(k,category) ** v)\n",
    "        classes_prob[category] *= class_priors[category]\n",
    "        #update with the prior class probability \n",
    "    return max(classes_prob, key = classes_prob.get)  \n",
    "\n",
    "def predict_class_dict(word_counter):\n",
    "    classes_prob = {}\n",
    "    for category in categories:\n",
    "        classes_prob[category] = 1\n",
    "        for k, v in word_counter.items():\n",
    "            classes_prob[category] *= (get_conditional(k,category) ** v)\n",
    "        classes_prob[category] *= class_priors[category]\n",
    "        #update with the prior class probability \n",
    "    return classes_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hockey': 2.849403069937408e-157,\n",
       " 'movies': 9.3257795979085874e-156,\n",
       " 'nba': 2.9819659321157092e-160,\n",
       " 'news': 3.971654743780464e-156,\n",
       " 'nfl': 4.541527357186871e-156,\n",
       " 'politics': 6.2294448461937019e-163,\n",
       " 'soccer': 1.2404713992481133e-142,\n",
       " 'worldnews': 2.2744387194795351e-158}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class_dict(test_input[\"words_count\"][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 77.7 ms, total: 17.4 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_output_series = test_input[\"words_count\"].apply(lambda x: predict_class(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 37.3 ms, total: 17.6 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_output_series_idf = test_input[\"words_count\"].apply(lambda x: predict_class_idf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_input) - sum(test_output_series == test_output_series_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_output = pd.DataFrame(test_input[\"id\"])\n",
    "test_output[\"category\"] = test_output_series\n",
    "test_output.to_csv(\"naive_bayes_prediction.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
